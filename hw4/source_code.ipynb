{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "premium",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xrH5ibW35-FX"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import string\n",
        "import re\n",
        "from unicodedata import normalize\n",
        "import numpy as np\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential,load_model\n",
        "from keras.layers import LSTM,Dense,Embedding,RepeatVector,TimeDistributed\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "import pandas as pd\n",
        "from string import punctuation\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "def printmd(string):\n",
        "    # Print with Markdowns    \n",
        "    display(Markdown(string))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "expr_path = \"/content/drive/MyDrive/NLP_Hw4_data/\"\n",
        "ot_tr_dataset = expr_path+\"jsonDataset.txt\"\n",
        "\n",
        "with open(ot_tr_dataset) as f:\n",
        "   data = json.load(f)\n",
        "\n",
        "dataset = data[\"dataset\"]\n",
        "\n",
        "#cleaning data and removes umwanted characters\n",
        "def str_cleaning(a_str):\n",
        "  a_str=a_str.lower()\n",
        "  a_str = a_str.replace(\"ı\",\"i\").replace(\"ü\",\"u\").replace(\".\",\"\").replace(\"ş\",\"s\").replace(\"ç\",\"c\").replace(\"ğ\",\"g\").replace(\"ö\",\"o\")\n",
        "  result = ''.join(c for c in a_str if c.isalpha() or c == \" \")\n",
        "  return re.sub(' +', ' ', result).rstrip()\n",
        "\n",
        "\n",
        "\n",
        "def make_dataframe_from_dataset(dataset,trashold=500):\n",
        "  ot_list = []\n",
        "  tr_list = []  \n",
        "  for sentence in dataset:\n",
        "    ot = sentence[\"translation\"][\"ot\"]\n",
        "    tr = sentence[\"translation\"][\"tr\"]\n",
        "    ot = str_cleaning(ot)\n",
        "    tr = str_cleaning(tr)\n",
        "    if len(ot)<trashold and (len(ot)!=0 and len(tr)!=0 ):\n",
        "\n",
        "\n",
        "\n",
        "      ot_list.append(ot)\n",
        "      tr_list.append(tr)\n",
        "\n",
        "  df = pd.DataFrame(\n",
        "      {   'Turkish': tr_list,\n",
        "          'Ottoman': ot_list\n",
        "      })\n",
        "\n",
        "  \n",
        "  return df \n",
        "\n"
      ],
      "metadata": {
        "id": "7Z1BgmCy7Zjl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = make_dataframe_from_dataset(dataset,100)\n",
        "\n",
        "df = df.sample(frac=1, random_state=0)\n",
        "print(len(df))\n",
        "df.iloc[1000:1010]\n"
      ],
      "metadata": {
        "id": "Jvscp9kwL_T7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_sentences = 40000\n",
        "\n",
        "# Select one part of the dataset\n",
        "dataset = df.values\n",
        "dataset = dataset[:total_sentences]\n",
        "\n",
        "# What proportion of the sentences will be used for the test set\n",
        "test_proportion = 0.1\n",
        "train_test_threshold = int( (1-test_proportion) * total_sentences)\n",
        "\n",
        "# split into train/test\n",
        "train, test = dataset[:train_test_threshold], dataset[train_test_threshold:]\n",
        "\n",
        "# Define the name of the source and of the target\n",
        "# This will be used in the outputs of this notebook\n",
        "source_str, target_str = \"Ottoman\", \"Turkish\"\n",
        "\n",
        "# The index in the numpy array of the source and of the target\n",
        "idx_src, idx_tar = 1, 0"
      ],
      "metadata": {
        "id": "DxWjwyn1FSJw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def create_tokenizer(lines):\n",
        "    # fit a tokenizer\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(lines)\n",
        "    return tokenizer\n",
        " \n",
        "def max_len(lines):\n",
        "    # max sentence length\n",
        "    return max(len(line.split()) for line in lines)\n",
        "\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "    # encode and pad sequences\n",
        "    X = tokenizer.texts_to_sequences(lines) # integer encode sequences\n",
        "    X = pad_sequences(X, maxlen=length, padding='post') # pad sequences with 0 values\n",
        "    return X\n",
        " \n",
        "def encode_output(sequences, vocab_size):\n",
        "    # one hot encode target sequence\n",
        "    ylist = list()\n",
        "    for sequence in sequences:\n",
        "        encoded = to_categorical(sequence, num_classes=vocab_size)\n",
        "        ylist.append(encoded)\n",
        "    y = np.array(ylist)\n",
        "    y = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
        "    return y\n",
        " \n",
        "# Prepare target tokenizer\n",
        "tar_tokenizer = create_tokenizer(dataset[:, idx_tar])\n",
        "tar_vocab_size = len(tar_tokenizer.word_index) + 1\n",
        "tar_length = max_len(dataset[:, idx_tar])\n",
        "printmd(f'\\nTarget ({target_str}) Vocabulary Size: {tar_vocab_size}')\n",
        "printmd(f'Target ({target_str}) Max Length: {tar_length}')\n",
        "\n",
        "# Prepare source tokenizer\n",
        "src_tokenizer = create_tokenizer(dataset[:, idx_src])\n",
        "src_vocab_size = len(src_tokenizer.word_index) + 1\n",
        "src_length = max_len(dataset[:, idx_src])\n",
        "printmd(f'\\nSource ({source_str}) Vocabulary Size: {src_vocab_size}')\n",
        "printmd(f'Source ({source_str}) Max Length: {src_length}\\n')\n",
        " \n",
        "# Prepare training data\n",
        "trainX = encode_sequences(src_tokenizer, src_length, train[:, idx_src])\n",
        "trainY = encode_sequences(tar_tokenizer, tar_length, train[:, idx_tar])\n",
        "trainY = encode_output(trainY, tar_vocab_size)\n",
        "\n",
        "# Prepare test data\n",
        "testX = encode_sequences(src_tokenizer, src_length, test[:, idx_src])\n",
        "testY = encode_sequences(tar_tokenizer, tar_length, test[:, idx_tar])\n",
        "testY = encode_output(testY, tar_vocab_size)"
      ],
      "metadata": {
        "id": "gp3KXuO_G1wU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def create_tokenizer(lines):\n",
        "    # fit a tokenizer\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(lines)\n",
        "    return tokenizer\n",
        " \n",
        "def max_len(lines):\n",
        "    # max sentence length\n",
        "    return max(len(line.split()) for line in lines)\n",
        "\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "    # encode and pad sequences\n",
        "    X = tokenizer.texts_to_sequences(lines) # integer encode sequences\n",
        "    X = pad_sequences(X, maxlen=length, padding='post') # pad sequences with 0 values\n",
        "    return X\n",
        " \n",
        "def encode_output(sequences, vocab_size):\n",
        "    # one hot encode target sequence\n",
        "    ylist = list()\n",
        "    for sequence in sequences:\n",
        "        encoded = to_categorical(sequence, num_classes=vocab_size)\n",
        "        ylist.append(encoded)\n",
        "    y = np.array(ylist)\n",
        "    y = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
        "    return y\n",
        " \n",
        "# Prepare target tokenizer\n",
        "tar_tokenizer = create_tokenizer(dataset[:, idx_tar])\n",
        "tar_vocab_size = len(tar_tokenizer.word_index) + 1\n",
        "tar_length = max_len(dataset[:, idx_tar])\n",
        "printmd(f'\\nTarget ({target_str}) Vocabulary Size: {tar_vocab_size}')\n",
        "printmd(f'Target ({target_str}) Max Length: {tar_length}')\n",
        "\n",
        "# Prepare source tokenizer\n",
        "src_tokenizer = create_tokenizer(dataset[:, idx_src])\n",
        "src_vocab_size = len(src_tokenizer.word_index) + 1\n",
        "src_length = max_len(dataset[:, idx_src])\n",
        "printmd(f'\\nSource ({source_str}) Vocabulary Size: {src_vocab_size}')\n",
        "printmd(f'Source ({source_str}) Max Length: {src_length}\\n')\n",
        " \n",
        "# Prepare training data\n",
        "trainX = encode_sequences(src_tokenizer, src_length, train[:, idx_src])\n",
        "trainY = encode_sequences(tar_tokenizer, tar_length, train[:, idx_tar])\n",
        "trainY = encode_output(trainY, tar_vocab_size)\n",
        "\n",
        "# Prepare test data\n",
        "testX = encode_sequences(src_tokenizer, src_length, test[:, idx_src])\n",
        "testY = encode_sequences(tar_tokenizer, tar_length, test[:, idx_tar])\n",
        "testY = encode_output(testY, tar_vocab_size)"
      ],
      "metadata": {
        "id": "aiWfWKRQHaff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
        "    # Create the model\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
        "    model.add(LSTM(n_units))\n",
        "    model.add(RepeatVector(tar_timesteps))\n",
        "    model.add(LSTM(n_units, return_sequences=True))\n",
        "    model.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
        "    return model\n",
        " \n",
        "# Create model\n",
        "model = create_model(src_vocab_size, tar_vocab_size, src_length, tar_length, 256)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "\n",
        "history = model.fit(trainX, \n",
        "          trainY, \n",
        "          epochs=200, \n",
        "          batch_size=64, \n",
        "          validation_split=0.1, \n",
        "          verbose=1,\n",
        "          callbacks=[\n",
        "                        EarlyStopping(\n",
        "                        monitor='val_loss',\n",
        "                        patience=10,\n",
        "                        restore_best_weights=True\n",
        "                    )\n",
        "            ])\n"
      ],
      "metadata": {
        "id": "5yGl5P7NMM0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(history.history).plot()\n",
        "plt.title(\"Loss\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Pr6W72a1MP2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def word_for_id(integer, tokenizer):\n",
        "    # map an integer to a word\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == integer:\n",
        "            return word\n",
        "    return None\n",
        " \n",
        "def predict_seq(model, tokenizer, source):\n",
        "    # generate target from a source sequence\n",
        "    prediction = model.predict(source, verbose=0)[0]\n",
        "    integers = [np.argmax(vector) for vector in prediction]\n",
        "    target = list()\n",
        "    for i in integers:\n",
        "        word = word_for_id(i, tokenizer)\n",
        "        if word is None:\n",
        "            break\n",
        "        target.append(word)\n",
        "    return ' '.join(target)\n",
        "\n",
        "def compare_prediction(model, tokenizer, sources, raw_dataset, limit=25):\n",
        "    # evaluate a model\n",
        "    actual, predicted = [], []\n",
        "    src = f'{source_str.upper()} (SOURCE)'\n",
        "    tgt = f'{target_str.upper()} (TARGET)'\n",
        "    pred = f'AUTOMATIC TRANSLATION IN {target_str.upper()}'\n",
        "    print(f'{src:50} {tgt:50} {pred}\\n')\n",
        "    \n",
        "    for i, source in enumerate(sources): # translate encoded source text\n",
        "        source = source.reshape((1, source.shape[0]))\n",
        "        translation = predict_seq(model, tar_tokenizer, source)\n",
        "        raw_target, raw_src = raw_dataset[i]\n",
        "        print(f'{raw_src:50} {raw_target:50} {translation}')\n",
        "        if i >= limit: # Display some of the result\n",
        "            break\n",
        " \n",
        "# test on some training sequences\n",
        "print('### Result on the Training Set ###')\n",
        "compare_prediction(model, tar_tokenizer, trainX, train)\n",
        "\n",
        "# test on some test sequences\n",
        "print('\\n\\n### Result on the Test Set ###')\n",
        "compare_prediction(model, tar_tokenizer, testX, test)\n"
      ],
      "metadata": {
        "id": "L4PwhbQnOHVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def bleu_score(model, tokenizer, sources, raw_dataset):\n",
        "    actual, predicted = [], []\n",
        "    for i, source in enumerate(sources):\n",
        "        source = source.reshape((1, source.shape[0]))\n",
        "        translation = predict_seq(model, tar_tokenizer, source)\n",
        "        raw_target, raw_src = raw_dataset[i]\n",
        "        actual.append([raw_target.split()])\n",
        "        predicted.append(translation.split())\n",
        "        \n",
        "    bleu_dic = {}\n",
        "    bleu_dic['1-grams'] = corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0))\n",
        "    bleu_dic['1-2-grams'] = corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0))\n",
        "    bleu_dic['1-3-grams'] = corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0))\n",
        "    bleu_dic['1-4-grams'] = corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25))\n",
        "    \n",
        "    return bleu_dic\n",
        "\n",
        "bleu_train = bleu_score(model, tar_tokenizer, trainX, train)\n",
        "bleu_test = bleu_score(model, tar_tokenizer, testX, test)"
      ],
      "metadata": {
        "id": "ZWRhNoNOOXs8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bleu_train"
      ],
      "metadata": {
        "id": "5DhpxRPBR1Jp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bleu_test\n"
      ],
      "metadata": {
        "id": "eqm-9NhxZU-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VLRivQEKZ3VD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}