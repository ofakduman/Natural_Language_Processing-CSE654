{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: '#download'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install git+https://github.com/ftkurt/python-syllable.git@master #download sylable repository\n",
    "from syllable import Encoder #import syllable repository\n",
    "\n",
    "encoder = Encoder(lang=\"tr\", limitby=\"vocabulary\", limit=3000)  # params chosen for demonstration purposes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bu en co de rin ru bir se kil de ça liş ti gi ni gös ter mek i cin bir or nek tir\n",
      "ha ya tin sa na ve re bi le ce gi bu tun ders le re gir\n",
      " yi li ga yet gu zel ti ben ce\n"
     ]
    }
   ],
   "source": [
    "#example about syllable encoder\n",
    "print(encoder.tokenize(\"Bu encoderin dogru bir sekilde çaliştigini göstermek icin bir ornektir\"))\n",
    "print(encoder.tokenize(\"hayatin sana verebilecegi butun derslere gir\"))\n",
    "print(encoder.tokenize(\"2022 yili ;gayet guzel gecti bence\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data preprocessing\n",
    "with open('wiki_00', encoding='utf8') as f: #read from file\n",
    "    full_str = f.read()\n",
    "\n",
    "with open('example.txt', 'w', encoding=\"utf8\") as f: #saved sample file to example.txt\n",
    "    f.write(full_str)\n",
    "\n",
    "with open('example.txt', encoding='utf8') as f: #\n",
    "    full_str = f.read()\n",
    "    \n",
    "# full_str = full_str[:100000]\n",
    "\n",
    "full_str = full_str[:(len(full_str)//3)]\n",
    "\n",
    "\n",
    "# Convert all the letters to small case letters first. You may convert all Turkish characters to\n",
    "#English ones. \n",
    "full_str=full_str.lower() #make small case character \n",
    "\n",
    "full_sentence = full_str.split(\".\")\n",
    "\n",
    "for i in range(len(full_sentence)):\n",
    "    full_sentence[i] = full_sentence[i].replace(\" \", \" spc \")\n",
    "    full_sentence[i] = encoder.tokenize(full_sentence[i])\n",
    "    #convert all Turkish characters to English ones.\n",
    "    full_sentence[i]=full_sentence[i].replace(\"ı\",\"i\").replace(\"ö\",\"o\").replace(\"ğ\",\"g\").replace(\"ç\",\"c\").replace(\"â\",\"a\").replace(\"ş\",\"s\").replace(\"ü\",\"u\").replace(\"\\n\",\"\")\n",
    "    new_txt = \"\"\n",
    "    for char in full_sentence[i]:\n",
    "        if char.isalpha() or char==\" \" or char==\".\":\n",
    "            new_txt+=char\n",
    "    full_sentence[i] = new_txt \n",
    "    full_sentence[i]=full_sentence[i].replace(\"    \",\"  \")\n",
    "    full_sentence[i]=full_sentence[i].replace(\"   \",\"  \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data processing 2\n",
    "#tokenizing process\n",
    "data = []\n",
    "for i in range(len(full_sentence)):\n",
    "    full_sentence[i] = full_sentence[i].replace(\"  \", \" space \")#to preserve space syllable\n",
    "    data.append(full_sentence[i].split(\" \"))    \n",
    "del full_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#data preprocessing 3\n",
    "\n",
    "#a method to make full txt sentence to ngram sentence\n",
    "def convert_ngram(full_sentence, n=3):\n",
    "    new_full_sentence = []\n",
    "    for i in range(len(full_sentence)):\n",
    "        new_full_sentence.append(ngram(full_sentence[i],n))\n",
    "    return new_full_sentence\n",
    "\n",
    "# a method to make ngram\n",
    "def ngram(alist, n=3):\n",
    "    new_list = []\n",
    "    word = \"\"\n",
    "    count = 0\n",
    "    for i in range(len(alist)):\n",
    "        \n",
    "        word += alist[i]\n",
    "        count +=1\n",
    "        if count == n:\n",
    "            new_list.append(word)\n",
    "            count = 0\n",
    "            word = \"\"\n",
    "            \n",
    "    return new_list\n",
    "    \n",
    "\n",
    "data_one_gram = convert_ngram(data,1)\n",
    "data_two_gram = convert_ngram(data,2)\n",
    "data_three_gram = convert_ngram(data,3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One gram example: ['', 'cen', 'giz', 'space', 'han', 'space', '']\n",
      "Two gram example: ['cen', 'gizspace', 'hanspace']\n",
      "Three gram example: ['cengiz', 'spacehanspace']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"One gram example\", end=\": \")\n",
    "print(data_one_gram[5])\n",
    "print(\"Two gram example\", end=\": \")\n",
    "print(data_two_gram[5])\n",
    "print(\"Three gram example\", end=\": \")\n",
    "print(data_three_gram[5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python program to generate word vectors using Word2Vec\n",
    "\n",
    "#to tokenize\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "#press the warnings\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action = 'ignore')\n",
    "\n",
    "#to use word2vec\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "\n",
    "#1gram\n",
    "# Create CBOW model\n",
    "model_1gram_cbow = gensim.models.Word2Vec(data_one_gram, min_count = 1,\n",
    "\t\t\t\t\t\t\tvector_size = 100, window = 3, workers=3)\n",
    "\n",
    "#Create Skip Gram model\n",
    "model_1gram_skipgram = gensim.models.Word2Vec(data_one_gram, min_count = 1, vector_size = 100,\n",
    "\t\t\t\t\t\t\t\t\t\t\twindow = 3, sg = 1, workers=3)\n",
    "\n",
    "\n",
    "#2gram\n",
    "# Create CBOW model\n",
    "model_2gram_cbow = gensim.models.Word2Vec(data_two_gram, min_count = 1,\n",
    "\t\t\t\t\t\t\tvector_size = 100, window = 3, workers=3)\n",
    "\n",
    "#Create Skip Gram model\n",
    "model_2gram_skipgram = gensim.models.Word2Vec(data_two_gram, min_count = 1, vector_size = 100,\n",
    "\t\t\t\t\t\t\t\t\t\t\twindow = 3, sg = 1, workers=3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-gram results\n",
      "Cosine similarity between 'la' and 'lar' - CBOW : \t 0.36677852\n",
      "Cosine similarity between 'len' and 'lan' - Skip Gram :  0.2978051\n",
      "Cosine similarity between 'da' and 'dan' - CBOW :\t 0.726438\n",
      "Cosine similarity between 'kem' and 'kum' - Skip Gram :\t 0.12815607\n"
     ]
    }
   ],
   "source": [
    "#Find the word similarity\n",
    "# Print results\n",
    "print(\"1-gram results\")\n",
    "print(\"Cosine similarity between 'la' \" +\n",
    "\t\t\"and 'lar' - CBOW : \\t\",\n",
    "\tmodel_1gram_cbow.wv.similarity('la', 'lar'))\n",
    "\t\n",
    "print(\"Cosine similarity between 'len' \" +\n",
    "\t\t\t\"and 'lan' - Skip Gram : \",\n",
    "\tmodel_1gram_skipgram.wv.similarity('len', 'lan'))\n",
    "\n",
    "print(\"Cosine similarity between 'da' \" +\n",
    "\t\t\"and 'dan' - CBOW :\\t\",\n",
    "\tmodel_1gram_cbow.wv.similarity('da', 'dan'))\n",
    "\t\n",
    "print(\"Cosine similarity between 'kem' \" +\n",
    "\t\t\t\"and 'kum' - Skip Gram :\\t\",\n",
    "\tmodel_1gram_skipgram.wv.similarity('kem', 'kum'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3-gram results\n",
      "Cosine similarity between 'dalari' and 'dalar ' - CBOW : \t 0.75750655\n",
      "Cosine similarity between 'raflarda' and 'raflardan' - CBOW :\t 0.742246\n",
      "Cosine similarity between 'deneme' and 'deme ' - Skip Gram :  0.6710553\n",
      "Cosine similarity between 'araba' and 'arada' - CBOW :\t 0.73925555\n",
      "Cosine similarity between 'arada' and 'deneme' - Skip Gram :\t 0.647212\n"
     ]
    }
   ],
   "source": [
    "#Find the word similarity\n",
    "# Print results\n",
    "print(\"3-gram results\")\n",
    "print(\"Cosine similarity between 'dalari' \" +\n",
    "\t\t\"and 'dalar ' - CBOW : \\t\",\n",
    "\tmodel_3gram_cbow.wv.similarity('dalari', 'dalarspace'))\n",
    "\n",
    "print(\"Cosine similarity between 'raflarda' \" +\n",
    "\t\t\"and 'raflardan' - CBOW :\\t\",\n",
    "\tmodel_3gram_cbow.wv.similarity('raflarda', 'raflardan'))\n",
    "\t\n",
    "print(\"Cosine similarity between 'deneme' \" +\n",
    "\t\t\t\"and 'deme ' - Skip Gram : \",\n",
    "\tmodel_3gram_skipgram.wv.similarity('deneme', 'demespace'))\n",
    "\n",
    "print(\"Cosine similarity between 'araba' \" +\n",
    "\t\t\"and 'arada' - CBOW :\\t\",\n",
    "\tmodel_3gram_cbow.wv.similarity('araba', 'arada'))\n",
    "\t\n",
    "print(\"Cosine similarity between 'arada' \" +\n",
    "\t\t\t\"and 'deneme' - Skip Gram :\\t\",\n",
    "\tmodel_3gram_skipgram.wv.similarity('arada', 'deneme'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between 'adama' and 'adami' - CBOW : \t 0.7325764\n",
      "Cosine similarity between 'kadina' and 'kadini' - CBOW :\t 0.7511469\n",
      "Cosine similarity between 'kapiya' and 'kapiyi' - Skip Gram :  0.9329534\n",
      "Cosine similarity between 'yapiya' and 'yapiyi' - Skip Gram :\t 0.7692217\n"
     ]
    }
   ],
   "source": [
    "# Print results\n",
    "print(\"Cosine similarity between 'adama' \" +\n",
    "\t\t\"and 'adami' - CBOW : \\t\",\n",
    "\tmodel_3gram_cbow.wv.similarity('adama', 'adami'))\n",
    "\n",
    "print(\"Cosine similarity between 'kadina' \" +\n",
    "\t\t\"and 'kadini' - CBOW :\\t\",\n",
    "\tmodel_3gram_cbow.wv.similarity('kadina', 'kadini'))\n",
    "\t\n",
    "print(\"Cosine similarity between 'kapiya' \" +\n",
    "\t\t\t\"and 'kapiyi' - Skip Gram : \",\n",
    "\tmodel_3gram_skipgram.wv.similarity('kapiya', 'kapiyi'))\n",
    "\n",
    "\t\n",
    "print(\"Cosine similarity between 'yapiya' \" +\n",
    "\t\t\t\"and 'yapiyi' - Skip Gram :\\t\",\n",
    "\tmodel_3gram_skipgram.wv.similarity('yapiya', 'yapiyi'))\n",
    "\n",
    "print(\"Cosine similarity between 'adama' \" +\n",
    "\t\t\"and 'kadina' - CBOW : \\t\",\n",
    "\tmodel_3gram_cbow.wv.similarity('adama', 'kadina'))\n",
    "\n",
    "print(\"Cosine similarity between 'adami' \" +\n",
    "\t\t\"and 'kadini' - CBOW :\\t\",\n",
    "\tmodel_3gram_cbow.wv.similarity('adami', 'kadini'))\n",
    "\t\n",
    "print(\"Cosine similarity between 'kapiya' \" +\n",
    "\t\t\t\"and 'yapiya' - Skip Gram : \",\n",
    "\tmodel_3gram_skipgram.wv.similarity('kapiya', 'yapiya'))\n",
    "\n",
    "\t\n",
    "print(\"Cosine similarity between 'kapiyi' \" +\n",
    "\t\t\t\"and 'yapiyi' - Skip Gram :\\t\",\n",
    "\tmodel_3gram_skipgram.wv.similarity('kapiyi', 'yapiyi'))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0 (main, Oct 24 2022, 18:26:48) [MSC v.1933 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "932ec4ae3fe1636a04c1f2dbe5802f894b0a5914f763ae03d6a1470cefbbbea1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
